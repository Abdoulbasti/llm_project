"""
Point d'entrÃ©e principal du projet LLM.
Pipeline automatique: Scraping (3) â†’ Nettoyage (4) â†’ Dataset (5)

Usage: python3 ./src/main.py
"""

from web_scraper import WebScraper
from scraping_sources_3 import SOURCES_CONFIG
import clean_text_4
import create_dataset_5


def main():
    """Pipeline complet: 3 â†’ 4 â†’ 5."""
    print("\n" + "=" * 60)
    print("PIPELINE LLM: Scraping â†’ Nettoyage â†’ Dataset")
    print("=" * 60 + "\n")

    # Ã‰tape 3: Scraping
    print("[1/3] Scraping des sources...")
    print("=" * 60)
    scraper = WebScraper(output_dir="src/data")
    res = scraper.scrape_multiple_sources(SOURCES_CONFIG)

    if res["succes"] == 0:
        print("\nâŒ Scraping Ã©chouÃ©. ArrÃªt du pipeline.\n")
        return False

    # Ã‰tape 4: Nettoyage
    print("\n[2/3] Nettoyage du texte...")
    print("=" * 60)
    try:
        corpus = clean_text_4.load_and_clean("src/data")
        clean_text_4.save_corpus(corpus)
        print("âœ… Nettoyage terminÃ©\n")
    except FileNotFoundError as e:
        print(f"\nâŒ {e}")
        print("ArrÃªt du pipeline.\n")
        return False

    # Ã‰tape 5: CrÃ©ation du dataset
    print("[3/3] CrÃ©ation du dataset...")
    print("=" * 60)
    try:
        dataset = create_dataset_5.create_dataset()
        print("âœ… Dataset crÃ©Ã©\n")
    except FileNotFoundError as e:
        print(f"\nâŒ {e}")
        print("ArrÃªt du pipeline.\n")
        return False

    # SuccÃ¨s
    print("=" * 60)
    print("âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS")
    print("=" * 60)
    print("\nFichiers gÃ©nÃ©rÃ©s:")
    print("  - src/data/*.txt (fichiers scrapÃ©s)")
    print("  - src/data/cleaned_corpus.txt (corpus nettoyÃ©)")
    print("  - src/dataset/ (dataset Hugging Face)")
    print("\nğŸ’¡ Prochaine Ã©tape: Tokenisation et fine-tuning\n")

    return True


if __name__ == "__main__":
    main()
