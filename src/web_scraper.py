"""
Module de scraping pour rÃ©cupÃ©rer du contenu web.
"""
import requests
from bs4 import BeautifulSoup
import os
from typing import Optional


class WebScraper:
    """Classe pour gÃ©rer le scraping de pages web."""

    def __init__(self, output_dir: str = "src/data"):
        """
        Initialise le scraper.

        Args:
            output_dir: Dossier oÃ¹ sauvegarder les fichiers scrapÃ©s
        """
        self.output_dir = output_dir
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        # CrÃ©er le dossier de sortie s'il n'existe pas
        self._ensure_output_dir_exists()

    def _ensure_output_dir_exists(self):
        """CrÃ©e le dossier de sortie s'il n'existe pas."""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            print(f"âœ… Dossier '{self.output_dir}' crÃ©Ã©.")

    def scrape_page(self, url: str, nom_fichier: str, description: Optional[str] = None) -> dict:
        """
        RÃ©cupÃ¨re le contenu d'une page web et le sauvegarde dans un fichier.

        Args:
            url: URL de la page Ã  scraper
            nom_fichier: Nom du fichier de sortie (relatif au output_dir)
            description: Description optionnelle de la source

        Returns:
            dict: {"success": bool, "skipped": bool}
        """
        print(f"\n{'='*60}")
        if description:
            print(f"ğŸ“„ {description}")
        print(f"ğŸ”— URL: {url}")
        print(f"ğŸ’¾ Fichier: {nom_fichier}")
        print(f"{'='*60}")

        # VÃ©rifier si le fichier existe dÃ©jÃ 
        chemin_fichier = os.path.join(self.output_dir, nom_fichier)
        if os.path.exists(chemin_fichier):
            print(f"â­ï¸  Fichier dÃ©jÃ  existant, scraping ignorÃ©")
            return {"success": True, "skipped": True}

        try:
            # 1. RÃ©cupÃ©ration du contenu
            print("â³ Connexion en cours...")
            reponse = requests.get(url, headers=self.headers, timeout=10)
            reponse.raise_for_status()

            # 2. Analyse du HTML
            print("ğŸ” Analyse du contenu...")
            soup = BeautifulSoup(reponse.content, 'html.parser')

            # 3. Extraction du texte (paragraphes)
            paragraphes = soup.find_all('p')
            print(f"ğŸ“ {len(paragraphes)} paragraphes trouvÃ©s")

            # 4. Sauvegarde dans le fichier
            with open(chemin_fichier, 'w', encoding='utf-8') as fichier:
                # Contenu seulement (pas de mÃ©tadonnÃ©es pour Ã©viter le bruit)
                for p in paragraphes:
                    texte_propre = p.get_text().strip()
                    if texte_propre:
                        fichier.write(texte_propre + "\n\n")

            print(f"âœ… SuccÃ¨s! Contenu sauvegardÃ© dans '{chemin_fichier}'")
            return {"success": True, "skipped": False}

        except requests.exceptions.Timeout:
            print(f"âŒ Erreur: Timeout lors de la connexion Ã  {url}")
            return {"success": False, "skipped": False}
        except requests.exceptions.RequestException as e:
            print(f"âŒ Erreur de connexion: {e}")
            return {"success": False, "skipped": False}
        except Exception as e:
            print(f"âŒ Erreur inattendue: {e}")
            return {"success": False, "skipped": False}

    def scrape_multiple_sources(self, sources: list) -> dict:
        """
        Scrape plusieurs sources.

        Args:
            sources: Liste de dictionnaires avec les clÃ©s 'url', 'nom_fichier', 'description'

        Returns:
            Dictionnaire avec les statistiques du scraping
        """
        print(f"\nğŸš€ DÃ©marrage du scraping de {len(sources)} source(s)...\n")

        resultats = {
            "total": len(sources),
            "succes": 0,
            "echecs": 0,
            "ignores": 0,
            "fichiers_crees": []
        }

        for i, source in enumerate(sources, 1):
            print(f"\n[{i}/{len(sources)}]")

            res = self.scrape_page(
                url=source["url"],
                nom_fichier=source["nom_fichier"],
                description=source.get("description")
            )

            if res["success"]:
                resultats["succes"] += 1
                if not res["skipped"]:
                    resultats["fichiers_crees"].append(source["nom_fichier"])
                else:
                    resultats["ignores"] += 1
            else:
                resultats["echecs"] += 1

        # Affichage du rÃ©sumÃ©
        print(f"\n{'='*60}")
        print("ğŸ“Š RÃ‰SUMÃ‰ DU SCRAPING")
        print(f"{'='*60}")
        print(f"âœ… SuccÃ¨s: {resultats['succes']}/{resultats['total']}")
        if resultats["ignores"] > 0:
            print(f"â­ï¸  IgnorÃ©s (dÃ©jÃ  existants): {resultats['ignores']}/{resultats['total']}")
        print(f"âŒ Ã‰checs: {resultats['echecs']}/{resultats['total']}")
        if resultats["fichiers_crees"]:
            print(f"\nğŸ“ Nouveaux fichiers crÃ©Ã©s dans '{self.output_dir}':")
            for fichier in resultats["fichiers_crees"]:
                print(f"   - {fichier}")
        print(f"{'='*60}\n")

        return resultats
